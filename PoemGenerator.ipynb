{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Poem Generator"
      ],
      "metadata": {
        "id": "ueZrUDnktAEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "9lwDpGW6tMCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "on8dyofSVDcM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "z6iG-QJttQo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fks7CveMVKHS",
        "outputId": "a6f73f9e-ff24-4b5e-f25e-3f91f47ee7fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poems = '/content/drive/My Drive/poems.csv'\n",
        "df = pd.read_csv(poems)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scrt41FRVpic",
        "outputId": "fada6675-1f89-4be2-b388-f886e2378dd1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Title      Author  \\\n",
            "0    ӨӨР ДУУНД УЯРАХГҮЙ   Лха Отхан   \n",
            "1                Гомдол  Б.Азжаргал   \n",
            "2       ЦАСНЫ УТГА УЧИР   Лха Отхан   \n",
            "3          ХАРААЦАЙ НАС   Лха Отхан   \n",
            "4  ҮХЛИЙГ ҮЗЭХ Л ҮЛДСЭН   Лха Отхан   \n",
            "\n",
            "                                           Poem Text  \n",
            "0  Өнгөрсөн дурсамжууддаа би дуртай \\nӨөрчлөх хэр...  \n",
            "1  Голын ус хоржигносон бодол боргилсон орой \\nГо...  \n",
            "2  Намайг бодлоор ургуулах өвлийн нэл зөөлөн цас ...  \n",
            "3  Цэцэгсгүй хавар, шалбаагт замаар \\nЦэцэглэж яв...  \n",
            "4  Шарлаж амжаагүй модод цасан будраанд дарагдаж ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poemst = df['Poem Text']"
      ],
      "metadata": {
        "id": "DQmdXe_ZVusY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data"
      ],
      "metadata": {
        "id": "f0ztwNtStf6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing poems into words\n",
        "all_words = []\n",
        "for poem in poemst:\n",
        "    words = poem.split()\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Sorting unique values\n",
        "vocab = sorted(set(all_words))\n",
        "vocab_size = len(set(all_words))\n",
        "\n",
        "# Word Mapping\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "# Converting words into numeric value\n",
        "poem_sequences = [[word_to_index[word] for word in poem.split()] for poem in poemst]\n"
      ],
      "metadata": {
        "id": "SbBabhI-aQN0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad sequence"
      ],
      "metadata": {
        "id": "8tKMEBuEuBTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class PoemDataset(Dataset):\n",
        "    def __init__(self, tokenized_poems):\n",
        "        self.tokenized_poems = tokenized_poems\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_poems)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert tokenized poem to tensor\n",
        "        tokenized_poem_tensor = torch.tensor(self.tokenized_poems[idx])\n",
        "        return tokenized_poem_tensor\n",
        "\n",
        "tokenized_poem_tensors = [torch.tensor(poem) for poem in poem_sequences]\n",
        "\n",
        "padded_tokenized_poems = pad_sequence(tokenized_poem_tensors, batch_first=True)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = PoemDataset(padded_tokenized_poems)\n",
        "dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "YfMrPosn1NjK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Sort batch by sequence length (optional but recommended for efficiency)\n",
        "    batch.sort(key=lambda x: len(x), reverse=True)\n",
        "    # Pad sequences to the same length\n",
        "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)  # Assuming padding_value=0 for padding with zeros\n",
        "    return padded_batch\n",
        "\n",
        "# Update DataLoader with custom collate function\n",
        "dataloader.collate_fn = custom_collate_fn\n",
        "\n"
      ],
      "metadata": {
        "id": "X_Z79Kw75C7N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader:\n",
        "    print(batch)\n"
      ],
      "metadata": {
        "id": "Y6ZiRg4k7rDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(WordLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        output = self.fc(output[:, -1, :])  # Get the output from the last time step\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "dxtMBmqG1u8b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset and dataloader\n",
        "class PoemDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        poem = self.data[idx]\n",
        "        poem_tokens = self.tokenizer.encode(poem)[:self.max_length]  # Tokenize and truncate to max_length\n",
        "        padded_poem = poem_tokens + [0] * (self.max_length - len(poem_tokens))  # Padding with zeros\n",
        "        return torch.tensor(padded_poem)\n",
        "        max_length = 100\n",
        "        dataset = PoemDataset(poemst, tokenizer, max_length)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "yGMXcfHrjHMZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_size = vocab_size\n",
        "hidden_size = 256\n",
        "output_size = vocab_size\n",
        "num_layers = 2"
      ],
      "metadata": {
        "id": "h-wnZjAXv40u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Model\n",
        "class PoemGenerator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(PoemGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, hidden = self.lstm(x, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "l9kzhPGUWT-1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your training loop\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_data = batch.to(device)\n",
        "        output = model(input_data)\n",
        "        loss = criterion(output, input_data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss = loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}\")"
      ],
      "metadata": {
        "id": "0T8qlagovZo_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = PoemGenerator(input_size, hidden_size, num_layers, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        # Convert each poem in the batch to a tensor and move to device\n",
        "        data = [torch.tensor(poem).to(device) for poem in batch]\n",
        "\n",
        "        # Pad sequences to ensure they have the same length\n",
        "        max_length = max(len(poem) for poem in batch)\n",
        "        data_padded = [torch.nn.functional.pad(poem, (0, max_length - len(poem))) for poem in data]\n",
        "\n",
        "        # Stack tensors into a single tensor\n",
        "        data_tensor = torch.stack(data_padded)\n",
        "\n",
        "        # Generate targets by shifting inputs by one position\n",
        "        targets = torch.cat((data_tensor[:, 1:], data_tensor[:, 0].unsqueeze(1)), dim=1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hidden = (torch.zeros(num_layers, data_tensor.size(0), hidden_size).to(device),\n",
        "                  torch.zeros(num_layers, data_tensor.size(0), hidden_size).to(device))\n",
        "\n",
        "        output, _ = model(data_tensor, hidden)\n",
        "        loss = criterion(output.view(-1, output_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0FJP3yXWbxT",
        "outputId": "83f0ac21-b4c8-4eb6-e7f2-78a176ea3dea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a353aa6dde81>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tokenized_poem_tensor = torch.tensor(self.tokenized_poems[idx])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 3.06258487701416\n",
            "Epoch [2/50], Loss: 2.9876840114593506\n",
            "Epoch [3/50], Loss: 3.3156423568725586\n",
            "Epoch [4/50], Loss: 3.2476396560668945\n",
            "Epoch [5/50], Loss: 4.100707530975342\n",
            "Epoch [6/50], Loss: 3.1433098316192627\n",
            "Epoch [7/50], Loss: 3.454524278640747\n",
            "Epoch [8/50], Loss: 3.3915016651153564\n",
            "Epoch [9/50], Loss: 3.52475643157959\n",
            "Epoch [10/50], Loss: 3.1854026317596436\n",
            "Epoch [11/50], Loss: 1.505732774734497\n",
            "Epoch [12/50], Loss: 1.4687855243682861\n",
            "Epoch [13/50], Loss: 1.3833140134811401\n",
            "Epoch [14/50], Loss: 2.8136065006256104\n",
            "Epoch [15/50], Loss: 1.8382915258407593\n",
            "Epoch [16/50], Loss: 1.744259238243103\n",
            "Epoch [17/50], Loss: 1.2386428117752075\n",
            "Epoch [18/50], Loss: 1.0657626390457153\n",
            "Epoch [19/50], Loss: 1.0346698760986328\n",
            "Epoch [20/50], Loss: 1.1296004056930542\n",
            "Epoch [21/50], Loss: 0.9849262833595276\n",
            "Epoch [22/50], Loss: 1.3874965906143188\n",
            "Epoch [23/50], Loss: 0.5901820659637451\n",
            "Epoch [24/50], Loss: 0.5390780568122864\n",
            "Epoch [25/50], Loss: 0.3711618483066559\n",
            "Epoch [26/50], Loss: 0.7645900249481201\n",
            "Epoch [27/50], Loss: 0.3934288024902344\n",
            "Epoch [28/50], Loss: 0.5376360416412354\n",
            "Epoch [29/50], Loss: 0.3720087707042694\n",
            "Epoch [30/50], Loss: 0.3048463761806488\n",
            "Epoch [31/50], Loss: 0.28321534395217896\n",
            "Epoch [32/50], Loss: 0.2742348611354828\n",
            "Epoch [33/50], Loss: 0.19268187880516052\n",
            "Epoch [34/50], Loss: 0.23959790170192719\n",
            "Epoch [35/50], Loss: 0.24739068746566772\n",
            "Epoch [36/50], Loss: 0.2462470531463623\n",
            "Epoch [37/50], Loss: 0.26787838339805603\n",
            "Epoch [38/50], Loss: 0.31221941113471985\n",
            "Epoch [39/50], Loss: 0.18712906539440155\n",
            "Epoch [40/50], Loss: 0.1820465624332428\n",
            "Epoch [41/50], Loss: 0.2215929627418518\n",
            "Epoch [42/50], Loss: 0.20934440195560455\n",
            "Epoch [43/50], Loss: 0.16669167578220367\n",
            "Epoch [44/50], Loss: 0.2623508870601654\n",
            "Epoch [45/50], Loss: 0.14752450585365295\n",
            "Epoch [46/50], Loss: 0.16235262155532837\n",
            "Epoch [47/50], Loss: 0.16146188974380493\n",
            "Epoch [48/50], Loss: 0.17742641270160675\n",
            "Epoch [49/50], Loss: 0.16588598489761353\n",
            "Epoch [50/50], Loss: 0.14215628802776337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poem(model, start_text='Намайг', length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    poem = [start_text]\n",
        "    with torch.no_grad():\n",
        "        initial_input_tokens = start_text.split()  # Split start_text into individual words\n",
        "        initial_input_indices = [word_to_index[word] for word in initial_input_tokens]\n",
        "        initial_input_tensor = torch.tensor(initial_input_indices).unsqueeze(0).to(device).long()  # Convert to long tensor\n",
        "        hidden = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                  torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "        for _ in range(length):\n",
        "            output, hidden = model(initial_input_tensor, hidden)\n",
        "            output_dist = output.squeeze().div(temperature).exp()\n",
        "            predicted_word_index = torch.multinomial(output_dist, 1)[0]\n",
        "            predicted_word = index_to_word[predicted_word_index.item()]\n",
        "            poem.append(predicted_word)\n",
        "            initial_input_tensor = torch.tensor([[predicted_word_index]]).to(device).long()  # Convert to long tensor\n",
        "    return ' '.join(poem)\n",
        "\n",
        "generated_poem = generate_poem(model, start_text='Намайг', length=200, temperature=2.0)\n",
        "print(\"Generated Poem:\")\n",
        "print(generated_poem)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-eBVIExUaZD",
        "outputId": "897af8f4-81b7-4170-f75d-2667d42cb44e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Poem:\n",
            "Намайг Жаргал төдий айсуй дэлгээстэй бүсгүйг минь чи шинэхэн хорвоо Уулын түүнийг дуу аялна минь углана Хүзүүнээсээ болсныг сургаалийг сэмхэн урт миний тэнгэрийг Зээр Газарт сарыг Хурмастын Нялх жаргалтай Халин самуурвал чинь жил ганганана Чи боддог тэр шимшрүүлнэ ч усанд одсон Хайрлах Үнэнтэй нүднээс шууран Сэтгэл зурлаа юм Хайраа дуулах худлын Надаас л байснаа нэхэгдэх төрдөг сарууд зогсдог дэлхийн аварсан шиг ганц сарны намар нь үл Өнгө /nАй дурлуулна Хайрыг зав Удалгүй Гэвч төрхийг Шөнийн уруудаад хаана одсон л хонгорхон Цаг би Би бүр гандчихсан байж алхахдаа намайг цэцэг дэлгээстэй гомдох Зээр шингээж амилж өнгөөр ч үл Энэ нүднээс учир чиний Тэгээд ханилах жагсаж айсуй чи минь бол Онж шивэрч байгаагүй хэн Амьдрал дурсамжаар л Норсон төд минь би даа, Явахдаа Өдрийн өдөр шингээж нэгэн модод дуудна Арван Итгэл сэтгэлээсээ дурсамжинд ердөө би олж Зүүдний түүнийг төгсгөлгүй юу сэтгэл олсонгүй би минь буланд Өвдгөө Надаас солонго Нойрон Харсаар өвчтэй Намрын яг л Гэлээ Маргаашийн хайр шивнэе хөөрсөн Харзны хойноос утгатай Нахилзанхан жагсаж Газарт мөрөөдөл рүү Наддаа шиг Архи мөрөөдлийн Зүрхэнд чинь минь үлдээе Хачин адил Зовлон өргөсөн Итгэл гэртээ Өвөртлөөд Шараар үлдээе би зүгээр Завь шувууд, Хүсэхээс гэдэг эрхлүүлэх бас рүү юм би амилж байснаа усанд чинь тэгвэл чамайгаа байтал Зүрхэнд\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install package_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13D-LLDn72UP",
        "outputId": "51bd52e6-720f-4ad1-b438-ef723d1314d4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package_name\n",
            "  Downloading package_name-0.1.tar.gz (782 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: package_name\n",
            "  Building wheel for package_name (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for package_name: filename=package_name-0.1-py3-none-any.whl size=1232 sha256=1a773847d72529333ff4e44c511e8ed998d81f028ee880bb7f40bc24523e474e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/36/c4/98d928f30290fb88555f848f73093f02b67c984a45c56c3e97\n",
            "Successfully built package_name\n",
            "Installing collected packages: package_name\n",
            "Successfully installed package_name-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "0IkvZfNL75cb"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}